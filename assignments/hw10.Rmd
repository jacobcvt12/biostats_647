---
title: "Homework 9"
author: "Jacob Carey"
date: \today
header-includes:
    - \usepackage{amsthm}
    - \usepackage{mathtools}
output: pdf_document
---

1.
    (i)
        Likelihood:
        MLE:
        Score function:
        Observed Fisher Information:
        Expected Fisher Information:

    (ii)
        Likelihood: $$\prod_{i=1}^n \frac{\theta^{x_i}e^{-\theta}}{x_i!}$$
        MLE: $$\frac{1}{n}\sum_{i=1}^n x_i$$
        Score function: $$-n + \frac{1}{\theta}\sum_{i=1}^n x_i$$
        Observed Fisher Information: $$\frac{1}{\theta^2}\sum_{i=1}^n x_i$$
        Expected Fisher Information: $$\frac{1}{\theta^2}E \Big[\sum x_i \Big]=\frac{n}{\theta}$$

    (iii)
        Likelihood:
        MLE:
        Score function:
        Observed Fisher Information:
        Expected Fisher Information:

2.
    (i)
    $$
    \prod (2\pi \sigma^2)^{-1/2} \exp \Big\{-1/2 \sigma^2 (y_i - \beta x_i)^2 \Big\} = (2\pi \sigma^2)^{-n/2} \exp \Big\{ -\frac{1}{2 \sigma^2} \sum (y_i-\beta x_i)^2 \Big\}
    $$

    (ii)
    $$
    \log \prod (2\pi \sigma^2)^{-1/2} \exp \Big\{-1/2 \sigma^2 (y_i - \beta x_i)^2 \Big\} = -\frac{n}{2}\log(2\pi \sigma^2)-\frac{1}{2 \sigma^2} \sum(y_i-\beta x_i)^2
    $$
    $$
    \frac{\partial \log f}{\partial \beta} = -\frac{1}{2\sigma^2}\sum 2(y_i - \beta x_i)(x_i) = \frac{\sum x_i (y_i - \beta x_i)}{\sigma^2}
    $$

    (iii)
    $$
    \begin{aligned}
    \frac{\sum x_i (y_i - \beta x_i)}{\sigma^2} &= 0 \\
    \sum x_i y_i - \beta x_i^2 &= 0 \\
    \beta = \frac{\sum x_i y_i}{\sum x_i^2}
    \end{aligned}
    $$

    (iv)
    $$
    \frac{\partial}{\partial \beta} \frac{\sum x_i y_i - \beta \sum x_i^2}{\sigma^2} = -\frac{\sum x_i^2}{\sigma^2} \implies J(\theta) =\frac{\sum x_i^2}{\sigma^2}
    $$

    (v)
    $$
    f(y; \hat{beta})=(2\pi\sigma^2)^{-\frac{n}{2}} \exp(-\frac{1}{2 \sigma^2}(\sum y_i^2 - \frac{3(\sum x_i y_i)^2}{\sum x_i^2})
    $$
    $$
    \frac{f(y; \beta)}{f(y; \hat{\beta})} = \exp\Big\{-\frac{1}{2 \sigma^2}\Big[\frac{3(\sum x_i y_i)^2}{\sum x_i^2}-2\beta \sum x_i y_i - \beta^2 \sum x_i^2\Big]\Big\}
    $$

    (vi)
    $$
    E[\hat{beta}]=\frac{1}{\sum x_i^2} \sum x_i E[y_i] = \beta
    $$
    $$
    \text{Var}[\hat{beta}]=\frac{1}{(\sum x_i^2)^2} \sum x_i^2 \text{Var}[y_i] = \frac{\sigma^2}{\sum x_i^2}
    $$
    Then the sampling distribution is $N(\beta, \frac{\sigma^2}{\sum x_i^2})$

    (vii)

3.
    $$
    \begin{aligned}
    P(X \geq k) &\leq \frac{E[X]}{k} \\
    P_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\geq k \Big) &\leq \frac{E_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\Big)}{k} \\
    E_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\Big) = \int_X f(x;\theta_0)\frac{f(x;\theta_1)}{f(x;\theta_0)}dx &=\int_X f(x;\theta_1) dx = 1 \\
    \implies P_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\geq k \Big) &\leq \frac{1}{k} \qedsymbol
    \end{aligned}
    $$

4. 
    $$
    \begin{aligned}
        \text{MSE}(\hat{\theta})&=E[(\hat{\theta}-\theta^2)] \\
        &=E[\hat{\theta}-E[\hat{\theta}]+E[\hat{\theta}]-\theta)^2] \\
        &=E[(\hat{\theta}-E[\hat{\theta}])^2]+2(E[\hat{\theta}]-\theta)E[\hat{\theta}-E(\hat{\theta})]+E[E(\hat{\theta}-\theta)^2] \\
        &= E[(\hat{\theta}-E[\hat{\theta}] + (E[\hat{\theta}]-\theta)^2 \\
        &= \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta}, \theta)^2 \qedsymbol
    \end{aligned}
    $$

5. (a)
    Joint density:
    $$
    \prod f(x_i; \theta) = \prod \frac{e^{-x_i/\theta}}{\theta} = \theta^{-n}\exp\{-\frac{1}{\theta}\sum x_i\}
    $$

    Score function:
    $$
    \begin{aligned}
    \log \prod f(x_i;\theta) &= \sum \log\{\theta^{-1}\exp(-x_i/\theta)\} \\
    &= -n \log \theta - \theta^{-1} \sum x_i \\
    \implies \frac{\partial \log f(x;\theta)}{\partial \theta} &= -\frac{n}{\theta} + \frac{n\bar{x}}{\theta^2}
    \end{aligned}
    $$
    (b)
    $$
    \begin{aligned}
    -\frac{n}{\theta}+\frac{n\bar{x}}{\theta^2} &= 0 \\
    \boxed{\hat{\theta}=\bar{x}}
    \end{aligned}
    $$
    Unbiased:
    $$
    E[\hat{\theta}] = E[\frac{1}{n}\sum x_i] = \frac{1}{n}\sum \theta = \theta
    $$
    Consistent:
    $$
    P(|1/n\sum x_i - \theta|\geq \epsilon) = P((1/n \sum x_i - \theta) \geq \epsilon^2) = E(1/n \sum x_i - \theta)^2/ \epsilon^2
    $$
    $$
    E(1/n\sum x_i-\theta)^2 = 0 \implies P(\hat{\theta} - \theta \geq \epsilon) = 0
    $$
    Variance:
    $$
    \text{Var}(\bar{X})=\frac{1}{n^2}\text{Var}(\sum X_i) = \frac{1}{n^2}(E([\sum x_i]^2)-[E(\sum x_i)]^2)=\frac{1}{n^2} \sum \text{Var}(X_i)
    $$
    (c)
    $$
    \frac{\theta^{-n}\exp(-\theta^{-1}\sum x_i)}{\hat{\theta}^{-n}\exp(-\hat{\theta}^{-1}\sum x_i)} = \Big(\frac{\hat{\theta}}{\theta}\Big)^n\exp(n \bar{x}(1/\bar{x}-1/\theta))=\Big(\frac{\hat{\theta}}{\theta}\Big)^n\exp\Big\{-n\Big(\frac{\hat{\theta}}{\theta}-1\Big)\Big\}
    $$

    (d)
    $$
    Y=n\hat{\theta}=n\bar{X}=\frac{n\sum X_i}{n}=S_n \sim \text{Gamma}(\alpha_1+...+\alpha_n, \beta)
    $$
    Then for $\alpha_i=1, \beta=\theta$
    $$
    S_n \sim \text{Gamma}(n, \beta)
    $$

    (e)
    $$
    P(Z \leq z)=P(Y \leq z \theta) = \frac{\gamma(n, z)}{\Gamma(n)} \implies f(z; \theta) = \frac{z^{n-1}e^{-z}}{\Gamma(n)}
    $$
    Then $\frac{n\hat{\theta}}{\theta} \sim \text{Gamma}(n, 1)$

    (f) Let $\gamma_p(\alpha, \beta)$ denote the $p-$th quantile of a gamma distribution. Then
    $$
    L=\gamma_{\alpha/2}(n, 1); U=\gamma_{1-\alpha/2}(n, 1)
    $$

    (g) Note that we can rewrite the 95% confidence interval as $P(n\hat{\theta}/U \leq \theta \leq n\hat{\theta}/L)$ where $L$ and $U$ represent the lower and upper limits of part (f) respectively. Then
    ```{r 5g1}
    y <- c(70, 11, 66, 5, 20, 4, 35, 40, 29, 8)
    n <- length(y)
    theta.hat <- mean(y)
    lower <- n * theta.hat / qgamma(0.975, n, 1)
    upper <- n * theta.hat / qgamma(0.025, n, 1)
    c(lower, upper)
    ```

    This confidence interval represents an interval such that in a large number of experiments, the interval would contain the truth.

    Here we computationally calculate the likelihood interval
    ```{r 5g2}
    lik <- function(theta, theta.hat, n) {
        return((theta.hat / theta) ^ n * exp(-n * (theta.hat / theta - 1)))
    }

    x <- seq(1, 100, by=0.01)
    likelihood <- sapply(x, lik, mean(y), length(y))
    dist <- abs(likelihood - 1 / 8)

    # sort data by distance from desired k=1/8
    data <- cbind(x, likelihood, dist)
    data <- data[order(dist), ]

    # grab first 10 values and then get min/max
    ci <- data[1:10, "x"]
    ci <- c(min(ci), max(ci))
    ci
    ```

    This $\theta$ represents the values where the evidence of the data is 1/8 that of the MLE or less.

6. (a)
    $$
    F_{Y_n}(y)=F_x(y)^n \implies f_{Y_n}(y)=nF_x(y)^{n-1}f_x(y)
    $$
    $$
    F_{Y_n}(y) = \begin{cases}
        0 & y < 0 \\
        (y/\theta)^n & 0 \leq y < \theta \\
        1 & y \geq 0
        \end{cases}
    $$
    $$
    f_{Y_n}(y)= n \frac{y^{n-1}}{\theta^n}
    $$

    (b)
    $$
    F_{Z_n}(z) = P(Z_n \leq z) = 1-P(Y_n \leq \theta-z/n) = 
    \begin{cases}
    0 & z \leq 0 \\
    1-(1-\frac{z}{n \theta})^n & 0 < z \leq n \theta \\
    1 & z > n \theta
    \end{cases}
    $$
    $$
    \implies \lim_{n\to \infty} 1 - (1+\frac{z}{n\theta})^n=1-e^{-\frac{z}{\theta}}
    $$
    which is the distribution of the exponential

    (c)
    ```{r 6c}
    set.seed(1)
    z.50 <- replicate(1000, 50 *(5 - max(runif(50, 0, 5))))
    x <- seq(0, 30, by=0.01)
    z <- dexp(x, 5)
    adj <- 200 / max(z)
    z <- z * adj
    hist(z.50, breaks=50)
    lines(x, z)
    ```

    (d)
    $$
    \log(\prod f(x_i; \theta)) = -\sum \log \theta = -n \log \theta
    $$
    $$
    \implies \frac{\partial \log f}{\partial \theta} = -\frac{n}{\theta}
    $$
    It follows that the MLE of $\theta$ is the maximum $X_1, ..., X_n$

    (e)
    $$
    P(T \leq t) = F_x(t \theta)^n = \begin{cases}
    0 & t \theta < 0 \\
    t^n & t\theta \in [0, \theta) \\
    1 & t \theta \geq 0
    \end{cases}
    $$
    Then we have that
    $F_T(\alpha^{1/n}) = \alpha$ and $F_T(1) = 1$. Thus $P(\alpha^{1/n} \leq T \leq)=1-\alpha$. Then the confidence interval is $(\hat{\theta}, \frac{\hat{\theta}}{\alpha^{1/n}})$

7. 
    ```{r 7}
    ci <- function(x, mean, var) {
        n <- length(x)
        factor <- qnorm(1 - 0.05 / 2) * sqrt(var) / sqrt(n)
        lower <- mean(x) - factor
        upper <- mean(x) + factor

        mean >= lower & mean(x) <= upper
    }

    mean(replicate(1000, ci(rnorm(25, 3, sqrt(2)), 3, 2)))
    ```
