---
title: "Homework 9"
author: "Jacob Carey"
date: \today
header-includes:
    - \usepackage{amsthm}
    - \usepackage{mathtools}
output: pdf_document
---

1.
    (i)
        Likelihood:
        MLE:
        Score function:
        Observed Fisher Information:
        Expected Fisher Information:

    (ii)
        Likelihood: $$\prod_{i=1}^n \frac{\theta^{x_i}e^{-\theta}}{x_i!}$$
        MLE: $$\frac{1}{n}\sum_{i=1}^n x_i$$
        Score function: $$-n + \frac{1}{\theta}\sum_{i=1}^n x_i$$
        Observed Fisher Information: $$\frac{1}{\theta^2}\sum_{i=1}^n x_i$$
        Expected Fisher Information: $$\frac{1}{\theta^2}E \Big[\sum x_i \Big]=\frac{n}{\theta}$$

    (iii)
        Likelihood:
        MLE:
        Score function:
        Observed Fisher Information:
        Expected Fisher Information:

2.
    (i)
    $$
    \prod (2\pi \sigma^2)^{-1/2} \exp \Big\{-1/2 \sigma^2 (y_i - \beta x_i)^2 \Big\} = (2\pi \sigma^2)^{-n/2} \exp \Big\{ -\frac{1}{2 \sigma^2} \sum (y_i-\beta x_i)^2 \Big\}
    $$

    (ii)
    $$
    \log \prod (2\pi \sigma^2)^{-1/2} \exp \Big\{-1/2 \sigma^2 (y_i - \beta x_i)^2 \Big\} = -\frac{n}{2}\log(2\pi \sigma^2)-\frac{1}{2 \sigma^2} \sum(y_i-\beta x_i)^2
    $$
    $$
    \frac{\partial \log f}{\partial \beta} = -\frac{1}{2\sigma^2}\sum 2(y_i - \beta x_i)(x_i) = \frac{\sum x_i (y_i - \beta x_i)}{\sigma^2}
    $$

    (iii)

    (iv)

    (v)

    (vi)

    (vii)

3.
    $$
    \begin{aligned}
    P(X \geq k) &\leq \frac{E[X]}{k} \\
    P_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\geq k \Big) &\leq \frac{E_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\Big)}{k} \\
    E_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\Big) = \int_X f(x;\theta_0)\frac{f(x;\theta_1)}{f(x;\theta_0)}dx &=\int_X f(x;\theta_1) dx = 1 \\
    \implies P_{\theta_0} \Big(\frac{f(x;\theta_1)}{f(x;\theta_0)}\geq k \Big) &\leq \frac{1}{k} \qedsymbol
    \end{aligned}
    $$

4. 
    $$
    \begin{aligned}
        \text{MSE}(\hat{\theta})&=E[(\hat{\theta}-\theta^2)] \\
        &=E[\hat{\theta}-E[\hat{\theta}]+E[\hat{\theta}]-\theta)^2] \\
        &=E[(\hat{\theta}-E[\hat{\theta}])^2]+2(E[\hat{\theta}]-\theta)E[\hat{\theta}-E(\hat{\theta})]+E[E(\hat{\theta}-\theta)^2] \\
        &= E[(\hat{\theta}-E[\hat{\theta}] + (E[\hat{\theta}]-\theta)^2 \\
        &= \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta}, \theta)^2 \qedsymbol
    \end{aligned}
    $$

5. (a)
    Joint density:
    $$
    \prod f(x_i; \theta) = \prod \frac{e^{-x_i/\theta}}{\theta} = \theta^{-1}\exp\{-\frac{1}{\theta}\sum x_i\}
    $$

    Score function:
    $$
    \begin{aligned}
    \log \prod f(x_i;\theta) &= \sum \log\{\theta^{-1}\exp(-x_i/\theta)\} \\
    &= -n \log \theta - \theta^{-1} \sum x_i \\
    \implies \frac{\partial \log f(x;\theta)}{\partial \theta} &= -\frac{n}{\theta} + \frac{n\bar{x}}{\theta^2}
    \end{aligned}
    $$
    (b)
    $$
    \begin{aligned}
    -\frac{n}{\theta}+\frac{n\bar{x}}{\theta^2} &= 0 \\
    \boxed{\hat{\theta}=\bar{x}}
    \end{aligned}
    $$
    Unbiased:
    $$
    E[\hat{\theta}] = E[\frac{1}{n}\sum x_i] = \frac{1}{n}\sum \theta = \theta
    $$
    Consistent:
    $$
    P(|1/n\sum x_i - \theta|\geq \epsilon) = P((1/n \sum x_i - \theta) \geq \epsilon^2) = E(1/n \sum x_i - \theta)^2/ \epsilon^2
    $$
    $$
    E(1/n\sum x_i-\theta)^2 = 0 \implies P(\hat{\theta} - \theta \geq \epsilon) = 0
    $$
    Variance:
    $$
    \text{Var}(\bar{X})=\frac{1}{n^2}\text{Var}(\sum X_i) = \frac{1}{n^2}(E([\sum x_i]^2)-[E(\sum x_i)]^2)=\frac{1}{n^2} \sum \text{Var}(X_i)
    $$
    (c)

    (d)

    (e)

    (f)

    (g)

6. (a)
    $$
    F_{Y_n}(y)=F_x(y)^n \implies f_{Y_n}(y)=nF_x(y)^{n-1}f_x(y)
    $$
    $$
    F_{Y_n}(y) = \begin{cases}
        0 & y < 0 \\
        (y/\theta)^n & 0 \leq y < \theta \\
        1 & y \geq 0
        \end{cases}
    $$
    $$
    f_{Y_n}(y)= n \frac{y^{n-1}}{\theta^n}
    $$

    (b)
    $$
    F_{Z_n}(z) = P(Z_n \leq z) = 1-P(Y_n \leq \theta-z/n) = 
    \begin{cases}
    0 & z \leq 0 \\
    1-(1-\frac{z}{n \theta})^n & 0 < z \leq n \theta \\
    1 & z > n \theta
    \end{cases}
    $$
    $$
    \implies \lim_{n\to \infty} 1 - (1+\frac{z}{n\theta})^n=1-e^{-\frac{z}{\theta}}
    $$
    which is the distribution of the exponential

    (c)
    ```{r 6c}
    set.seed(1)
    z.50 <- replicate(1000, 50 *(5 - max(runif(50, 0, 5))))
    x <- seq(0, 30, by=0.01)
    z <- dexp(x, 5)
    adj <- 200 / max(z)
    z <- z * adj
    hist(z.50, breaks=50)
    lines(x, z)
    ```

    (d)
    $$
    \log(\prod f(x_i; \theta)) = -\sum \log \theta = -n \log \theta
    $$
    $$
    \implies \frac{\partial \log f}{\partial \theta} = -\frac{n}{\theta}
    $$
    It follows that the MLE of $\theta$ is the maximum $X_1, ..., X_n$

7. 
    ```{r 7}
    ci <- function(x, mean, var) {
        n <- length(x)
        factor <- qnorm(1 - 0.05 / 2) * sqrt(var) / sqrt(n)
        lower <- mean(x) - factor
        upper <- mean(x) + factor

        mean >= lower & mean(x) <= upper
    }

    mean(replicate(1000, ci(rnorm(25, 3, sqrt(2)), 3, 2)))
    ```
